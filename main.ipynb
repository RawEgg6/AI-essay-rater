{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b63c81e4",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# üìö Automated Essay Scoring with BERT\n",
    "\n",
    "This notebook trains a BERT-based model to automatically score student essays using the ASAP (Automated Student Assessment Prize) dataset.\n",
    "\n",
    "## üéØ Project Overview\n",
    "- **Dataset**: ASAP Essay Scoring Dataset (12,976 essays)\n",
    "- **Task**: Regression (predicting essay scores 0-1 scaled)\n",
    "- **Model**: BERT-base-uncased fine-tuned for sequence classification\n",
    "- **Evaluation**: QWK, Pearson Correlation, MAE, RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eab52c",
   "metadata": {},
   "source": [
    "### üìÇ Data Loading and Initial Exploration\n",
    "\n",
    "- This section loads the Automated Student Assessment Prize (ASAP) essay dataset from a TSV file. \n",
    "- It reads the data using pandas, retains only the essential columns for essay scoring \n",
    "- essay_id, essay_set, essay, domain1_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69728437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12976, 4)\n",
      "   essay_id  essay_set                                              essay  \\\n",
      "0         1          1  Dear local newspaper, I think effects computer...   \n",
      "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
      "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
      "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
      "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
      "\n",
      "   domain1_score  \n",
      "0              8  \n",
      "1              9  \n",
      "2              7  \n",
      "3             10  \n",
      "4              8  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset (use the TSV from Kaggle)\n",
    "df = pd.read_csv(\"training_set_rel3.tsv\", sep=\"\\t\", encoding=\"latin1\")\n",
    "\n",
    "# Keep relevant columns\n",
    "df = df[['essay_id', 'essay_set', 'essay', 'domain1_score']]\n",
    "\n",
    "# Display basic info\n",
    "print(df.shape)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a203b4f",
   "metadata": {},
   "source": [
    "### üßπ Data Cleaning and Filtering\n",
    "\n",
    "This section ensures the dataset contains only valid and readable essays before training.\n",
    "\n",
    "- Removes **illegible or corrupted essays** containing terms like `\"illegible\"` or `\"???\"`.  \n",
    "- Drops rows with **missing essay text or missing scores** to avoid null-related issues during training.  \n",
    "- **Resets the DataFrame index** after cleaning for proper alignment.  \n",
    "- Prints the **updated dataset shape** to confirm the final count of usable samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce906dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning, dataset shape: (12703, 4)\n"
     ]
    }
   ],
   "source": [
    "# Remove illegible or corrupted essays\n",
    "df = df[~df['essay'].str.contains('illegible|\\\\?\\\\?\\\\?', case=False, na=False)]\n",
    "\n",
    "# Drop any rows with missing text or scores\n",
    "df = df.dropna(subset=['essay', 'domain1_score']).reset_index(drop=True)\n",
    "print(f\"After cleaning, dataset shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c9176",
   "metadata": {},
   "source": [
    "### ‚öñÔ∏è Score Normalization Across Essay Sets\n",
    "\n",
    "This section standardizes essay scores within each essay set to ensure fair comparison during model training.\n",
    "\n",
    "- Iterates through all **essay sets** to display their **minimum and maximum raw scores**.  \n",
    "- Applies **min‚Äìmax normalization** within each essay set to rescale scores between **0 and 1**.  \n",
    "- Creates a new column called **`score_scaled`** representing the normalized scores.  \n",
    "- Prints a preview of the scaled scores alongside original values for verification.\n",
    "\n",
    "- **Formula**:\n",
    "\n",
    "\\[\n",
    "\\text{score\\_scaled} = \\frac{\\text{domain1\\_score} - \\text{min\\_score\\_of\\_set}}{\\text{max\\_score\\_of\\_set} - \\text{min\\_score\\_of\\_set}}\n",
    "\\]\n",
    "\n",
    "Where:  \n",
    "- `domain1_score` ‚Üí the original essay score  \n",
    "- `min_score_of_set` ‚Üí minimum score in the essay‚Äôs set  \n",
    "- `max_score_of_set` ‚Üí maximum score in the essay‚Äôs set  \n",
    "\n",
    "- **Implementation in Code**: Applies the formula row-wise using `df.apply` in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0051887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1: min=2, max=12\n",
      "Set 2: min=1, max=6\n",
      "Set 3: min=0, max=3\n",
      "Set 4: min=0, max=3\n",
      "Set 5: min=0, max=4\n",
      "Set 6: min=0, max=4\n",
      "Set 7: min=2, max=24\n",
      "Set 8: min=10, max=60\n",
      "==================================================\n",
      "   essay_set  domain1_score  score_scaled\n",
      "0          1              8           0.6\n",
      "1          1              9           0.7\n",
      "2          1              7           0.5\n",
      "3          1             10           0.8\n",
      "4          1              8           0.6\n"
     ]
    }
   ],
   "source": [
    "# Check score ranges per essay set\n",
    "for s in sorted(df['essay_set'].unique()):\n",
    "    min_score = df[df['essay_set'] == s]['domain1_score'].min()\n",
    "    max_score = df[df['essay_set'] == s]['domain1_score'].max()\n",
    "    print(f\"Set {s}: min={min_score}, max={max_score}\")\n",
    "\n",
    "print(\"=\" *50)    \n",
    "\n",
    "# Scale scores to 0-1 range per essay set\n",
    "df['score_scaled'] = df.apply(\n",
    "    lambda x: (x['domain1_score'] - df[df['essay_set'] == x['essay_set']]['domain1_score'].min()) /\n",
    "              (df[df['essay_set'] == x['essay_set']]['domain1_score'].max() -\n",
    "               df[df['essay_set'] == x['essay_set']]['domain1_score'].min()),\n",
    "    axis=1\n",
    ")\n",
    "print(df[['essay_set', 'domain1_score', 'score_scaled']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5951859",
   "metadata": {},
   "source": [
    "### ‚ú® Text Preprocessing and Cleaning\n",
    "\n",
    "This section defines and applies a function to clean essay text for model training.\n",
    "\n",
    "- **Define `clean_text` function** to standardize essay content.  \n",
    "- **Remove HTML tags** (`<...>`) using regular expressions.  \n",
    "- **Remove special characters** leaving only letters, numbers, and spaces.  \n",
    "- **Condense multiple spaces** into one and remove leading/trailing spaces.  \n",
    "- **Apply the cleaning function** to the `essay` column of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b27011bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)  # remove HTML\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # remove special chars\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # remove extra spaces\n",
    "    return text\n",
    "\n",
    "df['essay'] = df['essay'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1358db3",
   "metadata": {},
   "source": [
    "### üß™ Train‚ÄìValidation Split\n",
    "\n",
    "This section splits the dataset into **training and validation sets** for model evaluation.\n",
    "\n",
    "- **Import `train_test_split`** from `scikit-learn` to split data randomly.  \n",
    "- **Split essays and their scaled scores** into training (90%) and validation (10%) sets.  \n",
    "- **Set a random seed (`random_state=42`)** for reproducible splits.  \n",
    "- **Print sizes** of training and validation sets to verify the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e08cf688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 11432, Validation set size: 1271\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, val_texts, train_scores, val_scores = train_test_split(\n",
    "    df['essay'], df['score_scaled'], test_size=0.1, random_state=42\n",
    ")\n",
    "print(f\"Training set size: {len(train_texts)}, Validation set size: {len(val_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0511a2",
   "metadata": {},
   "source": [
    "### üî§ Tokenization with BERT\n",
    "\n",
    "This section converts essay text into **token IDs** suitable for BERT input.\n",
    "\n",
    "- **Import `AutoTokenizer`** from Hugging Face Transformers.  \n",
    "- **Load the BERT tokenizer** (`bert-base-uncased`) to match the pretrained model.  \n",
    "- **Tokenize training essays** with:  \n",
    "  - `truncation=True` ‚Üí cut essays longer than 512 tokens  \n",
    "  - `padding=True` ‚Üí pad shorter essays to the same length  \n",
    "  - `return_tensors='pt'` ‚Üí return PyTorch tensors  \n",
    "- **Repeat tokenization for validation essays** to prepare inputs for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0a81b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN. MODEL IS ALREADY TRAINED\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(\n",
    "    list(train_texts), truncation=True, padding=True, max_length=512, return_tensors='pt'\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "    list(val_texts), truncation=True, padding=True, max_length=512, return_tensors='pt'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca746032",
   "metadata": {},
   "source": [
    "### üì¶ Creating a Custom PyTorch Dataset\n",
    "\n",
    "This section defines a **custom dataset class** to feed tokenized essays and scores into a BERT model.\n",
    "\n",
    "- **Import `torch`** to use PyTorch utilities.  \n",
    "- **Define `EssayDataset` class** inheriting from `torch.utils.data.Dataset`:  \n",
    "  - `__init__` stores **tokenized encodings** and **labels**.  \n",
    "  - `__getitem__` returns a single data point as a dictionary with:  \n",
    "    - token IDs, attention masks, and other encodings  \n",
    "    - the corresponding **label** converted to a float tensor  \n",
    "  - `__len__` returns the total number of samples.  \n",
    "- **Create training and validation datasets** (`train_dataset`, `val_dataset`) from tokenized inputs and scaled scores, ready for PyTorch DataLoader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14679271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN. MODEL IS ALREADY TRAINED\n",
    "\n",
    "import torch\n",
    "\n",
    "class EssayDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = EssayDataset(train_encodings, train_scores)\n",
    "val_dataset = EssayDataset(val_encodings, val_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3546d13c",
   "metadata": {},
   "source": [
    "### üè∑Ô∏è Model Training with Hugging Face Trainer\n",
    "\n",
    "This cell sets up and runs the training process for the BERT-based essay scoring model using Hugging Face's `Trainer` API.  \n",
    "Key steps:\n",
    "\n",
    "- **Model Loading:** Loads a BERT model for regression (`num_labels=1`).\n",
    "- **Training Arguments:** Configures training parameters such as:\n",
    "  - Output directory for results and logs\n",
    "  - Number of epochs\n",
    "  - Batch sizes for training and evaluation\n",
    "  - Warmup steps and weight decay for optimization\n",
    "  - Evaluation and saving strategies (per epoch)\n",
    "  - Learning rate\n",
    "  - Apple Silicon GPU support (`use_mps_device=True`)\n",
    "- **Trainer Initialization:** Combines the model, training arguments, training dataset, and validation dataset.\n",
    "- **Model Training:** Runs the training loop with `trainer.train()`.\n",
    "\n",
    "This cell fine-tunes the BERT model on your essay data for automated scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcbb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN. MODEL IS ALREADY TRAINED\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "\n",
    "# Load model for REGRESSION (num_labels=1 for regression tasks)\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=1  # For regression (predicting continuous scores)\n",
    ")\n",
    "\n",
    "# Use 'eval_strategy' instead of 'evaluation_strategy'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    "    learning_rate=2e-5,\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    use_mps_device=True,  # Enable Apple Silicon GPU\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628496b",
   "metadata": {},
   "source": [
    "### üì¶ Model Loading and Evaluation Setup\n",
    "\n",
    "This cell loads the previously trained BERT model and tokenizer from the saved directory (`./essay_scoring_model`).  \n",
    "It then prepares the validation data for evaluation:\n",
    "\n",
    "- **Model & Tokenizer Loading:** Loads the fine-tuned BERT model and its tokenizer.\n",
    "- **Validation Encoding:** Tokenizes the validation essays (`val_texts`) to create PyTorch tensors for input.\n",
    "- **Custom Dataset:** Defines the `EssayDataset` class to wrap tokenized inputs and labels for PyTorch.\n",
    "- **Validation Dataset:** Instantiates the validation dataset using tokenized essays and their scores.\n",
    "- **Trainer Setup:** Re-instantiates the Hugging Face `Trainer` with the loaded model and validation dataset for evaluation or inference.\n",
    "\n",
    "This cell is used for running predictions and evaluating model performance, not for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e2786b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './essay_scoring_model'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:160\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must use alphanumeric chars or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m--\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m\u001b[33m are\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m forbidden, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m cannot start or end the name, max length is 96:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m--\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './essay_scoring_model'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load model and tokenizer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./essay_scoring_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m'\u001b[39m\u001b[33m./essay_scoring_model\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Re-create validation encodings and dataset\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:508\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    506\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[32m    507\u001b[39m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m         resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m         commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/hub.py:531\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    525\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPermissionError at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m when downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    526\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCheck cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m2) a previous download was canceled and the lock file needs manual removal.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    528\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m resolved_files = \u001b[43m[\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_get_cache_file_to_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfull_filenames\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved_files\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/hub.py:532\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    525\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPermissionError at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m when downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    526\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCheck cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m2) a previous download was canceled and the lock file needs manual removal.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    528\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[32m    531\u001b[39m resolved_files = [\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     \u001b[43m_get_cache_file_to_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[32m    534\u001b[39m ]\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved_files\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/hub.py:143\u001b[39m, in \u001b[36m_get_cache_file_to_return\u001b[39m\u001b[34m(path_or_repo_id, full_filename, cache_dir, revision, repo_type)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cache_file_to_return\u001b[39m(\n\u001b[32m    136\u001b[39m     path_or_repo_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    137\u001b[39m     full_filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    141\u001b[39m ):\n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# We try to see if we have a cached version (not up to date):\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     resolved_file = \u001b[43mtry_to_load_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resolved_file != _CACHED_NO_EXIST:\n\u001b[32m    147\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mzip\u001b[39m(signature.parameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[32m    103\u001b[39m     kwargs.items(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[32m    104\u001b[39m ):\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    109\u001b[39m         has_token = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:160\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    155\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must be in the form \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrepo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnamespace/repo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Use `repo_type` argument if needed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m     )\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must use alphanumeric chars or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m--\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m\u001b[33m are\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m forbidden, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m cannot start or end the name, max length is 96:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m--\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot have -- or .. in repo_id: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './essay_scoring_model'."
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./essay_scoring_model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./essay_scoring_model')\n",
    "\n",
    "# Re-create validation encodings and dataset\n",
    "val_encodings = tokenizer(\n",
    "    list(val_texts), truncation=True, padding=True, max_length=512, return_tensors='pt'\n",
    ")\n",
    "\n",
    "class EssayDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx], dtype=torch.float)\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "val_dataset = EssayDataset(val_encodings, val_scores)\n",
    "\n",
    "# Re-instantiate Trainer with loaded model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    eval_dataset=val_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd909773",
   "metadata": {},
   "source": [
    "### üìä Model Evaluation and Metrics Calculation\n",
    "\n",
    "This cell runs the evaluation of the loaded BERT model on the validation dataset and calculates key performance metrics:\n",
    "\n",
    "- **Predictions:** Uses the Hugging Face `Trainer` to predict scores for the validation essays.\n",
    "- **Rescaling:** Converts predicted and actual scores from the normalized (0‚Äì1) scale back to their original ranges for each essay set.\n",
    "- **Metrics Computation:** Calculates several metrics to assess model performance:\n",
    "  - Quadratic Weighted Kappa (QWK)\n",
    "  - Pearson Correlation\n",
    "  - R¬≤ Score\n",
    "  - Mean Absolute Error (MAE)\n",
    "  - Root Mean Squared Error (RMSE)\n",
    "- **Printing Results:** Displays the computed metrics for easy review.\n",
    "\n",
    "This cell helps you understand how well your BERT model predicts essay scores compared to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e34f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get predictions\n",
    "predictions = trainer.predict(val_dataset)\n",
    "preds = predictions.predictions.squeeze()\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# For QWK, we need to convert scaled scores back to original scale\n",
    "# Let's get the essay_set for each validation sample\n",
    "val_essay_sets = df.loc[val_texts.index, 'essay_set'].values\n",
    "\n",
    "def rescale_predictions(scaled_preds, scaled_labels, essay_sets):\n",
    "    \"\"\"Convert 0-1 scaled scores back to original scale\"\"\"\n",
    "    score_ranges = {\n",
    "        1: (2, 12), 2: (1, 6), 3: (0, 3), 4: (0, 3),\n",
    "        5: (0, 4), 6: (0, 4), 7: (2, 24), 8: (10, 60)\n",
    "    }\n",
    "    \n",
    "    original_preds = []\n",
    "    original_labels = []\n",
    "    \n",
    "    for pred, label, essay_set in zip(scaled_preds, scaled_labels, essay_sets):\n",
    "        min_score, max_score = score_ranges[essay_set]\n",
    "        original_pred = pred * (max_score - min_score) + min_score\n",
    "        original_label = label * (max_score - min_score) + min_score\n",
    "        original_preds.append(round(original_pred))\n",
    "        original_labels.append(round(original_label))\n",
    "    \n",
    "    return np.array(original_preds), np.array(original_labels)\n",
    "\n",
    "# Rescale predictions\n",
    "original_preds, original_labels = rescale_predictions(preds, labels, val_essay_sets)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_b = mean_absolute_error(labels, preds)\n",
    "rmse_b = np.sqrt(mean_squared_error(labels, preds))\n",
    "r2_b = r2_score(labels, preds)\n",
    "pearson_corr_b, _ = pearsonr(preds, labels)\n",
    "\n",
    "# QWK (on original scale)\n",
    "qwk_b = cohen_kappa_score(original_labels, original_preds, weights='quadratic')\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"üìä MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Quadratic Weighted Kappa (QWK): {qwk_b:.4f}\")\n",
    "print(f\"Pearson Correlation:            {pearson_corr_b:.4f}\")\n",
    "print(f\"R¬≤ Score:                       {r2_b:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE):      {mae_b:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_b:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0147dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# TF-IDF features (unigrams + bigrams)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 3),\n",
    "    \n",
    ")\n",
    "X_tfidf = vectorizer.fit_transform(df['essay'])\n",
    "\n",
    "# Simple numeric features\n",
    "df['essay_len'] = df['essay'].apply(len)\n",
    "df['word_count'] = df['essay'].apply(lambda x: len(x.split()))\n",
    "df['avg_word_len'] = df['essay'].apply(lambda x: np.mean([len(w) for w in x.split()]) if len(x.split()) > 0 else 0)\n",
    "\n",
    "numeric_features = df[['essay_len', 'word_count', 'avg_word_len']].values\n",
    "\n",
    "# Combine TF-IDF + numeric\n",
    "from scipy.sparse import csr_matrix\n",
    "X = hstack([X_tfidf, csr_matrix(numeric_features)])\n",
    "y = df['score_scaled'].values\n",
    "\n",
    "print(f\"Final feature matrix shape: {X.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1178ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Validation shape: {X_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b1fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, cohen_kappa_score\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "# Train Ridge Regression\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = ridge.predict(X_val)\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "pearson_corr, _ = pearsonr(y_pred, y_val)\n",
    "\n",
    "# Rescale predictions and labels for QWK\n",
    "score_ranges = {\n",
    "    1: (2, 12), 2: (1, 6), 3: (0, 3), 4: (0, 3),\n",
    "    5: (0, 4), 6: (0, 4), 7: (2, 24), 8: (10, 60)\n",
    "}\n",
    "\n",
    "def rescale(scaled, essay_sets):\n",
    "    original = []\n",
    "    for val, s in zip(scaled, essay_sets):\n",
    "        min_score, max_score = score_ranges[s]\n",
    "        orig = val * (max_score - min_score) + min_score\n",
    "        original.append(round(orig))\n",
    "    return np.array(original)\n",
    "\n",
    "# Get essay_set for each validation sample\n",
    "val_essay_sets = df.iloc[X_val.indices]['essay_set'].values if hasattr(X_val, 'indices') else df.iloc[X_val.nonzero()[0]]['essay_set'].values\n",
    "\n",
    "y_pred_orig = rescale(y_pred, val_essay_sets)\n",
    "y_val_orig = rescale(y_val, val_essay_sets)\n",
    "\n",
    "# Calculate QWK\n",
    "qwk = cohen_kappa_score(y_val_orig, y_pred_orig, weights='quadratic')\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Quadratic Weighted Kappa (QWK)',\n",
    "        'Pearson Correlation',\n",
    "        'R¬≤ Score',\n",
    "        'Mean Absolute Error (MAE)',\n",
    "        'Root Mean Squared Error (RMSE)'\n",
    "    ],\n",
    "    'Ridge': [\n",
    "        qwk,\n",
    "        pearson_corr,\n",
    "        r2,\n",
    "        mae,\n",
    "        rmse\n",
    "    ],\n",
    "    'BERT': [\n",
    "        qwk_b,\n",
    "        pearson_corr_b,\n",
    "        r2_b,\n",
    "        mae_b,\n",
    "        rmse_b\n",
    "    ]\n",
    "})\n",
    "\n",
    "metrics_df.to_csv('model_eval_metrics.csv', index=False)\n",
    "print(\"‚úÖ Saved evaluation metrics to 'model_eval_metrics.csv'\")\n",
    "\n",
    "print(\"=\" * 54)\n",
    "print(\"üìä MODEL PERFORMANCE METRICS         RIDGE\\t BERT\"    )\n",
    "print(\"=\" * 54)\n",
    "print(f\"Quadratic Weighted Kappa (QWK):     {qwk:.4f}\\t{qwk_b:.4f}\")\n",
    "print(f\"Pearson Correlation:                {pearson_corr:.4f}\\t{pearson_corr_b:.4f}\")\n",
    "print(f\"R¬≤ Score:                           {r2:.4f}\\t{r2_b:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE):          {mae:.4f}\\t{mae_b:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE):     {rmse:.4f}\\t{rmse_b:.4f}\")\n",
    "print(\"=\" * 54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b45f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Increased figure size for better readability\n",
    "fig, axes = plt.subplots(2, 4, figsize=(24, 14))\n",
    "\n",
    "# Increase marker size and adjust alpha for better visibility\n",
    "marker_size = 50\n",
    "alpha_val = 0.6\n",
    "\n",
    "# 1a. Scatter Plot: Predicted vs Actual (Scaled) - BERT\n",
    "axes[0, 0].scatter(labels, preds, alpha=alpha_val, s=marker_size)\n",
    "axes[0, 0].plot([0, 1], [0, 1], 'r--', lw=2.5, label='Perfect Prediction')\n",
    "axes[0, 0].set_xlabel('Actual Score (Scaled)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Predicted Score (Scaled)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_title(f'BERT: Predicted vs Actual\\nPearson r = {pearson_corr_b:.4f}', fontsize=15, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=12)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].tick_params(labelsize=12)\n",
    "\n",
    "# 1b. Scatter Plot: Predicted vs Actual (Scaled) - Ridge\n",
    "axes[0, 1].scatter(y_val, y_pred, alpha=alpha_val, s=marker_size)\n",
    "axes[0, 1].plot([0, 1], [0, 1], 'r--', lw=2.5, label='Perfect Prediction')\n",
    "axes[0, 1].set_xlabel('Actual Score (Scaled)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Predicted Score (Scaled)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_title(f'Ridge: Predicted vs Actual\\nPearson r = {pearson_corr:.4f}', fontsize=15, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=12)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].tick_params(labelsize=12)\n",
    "\n",
    "# 2a. Residual Plot - BERT\n",
    "residuals_bert = preds - labels\n",
    "axes[0, 2].scatter(labels, residuals_bert, alpha=alpha_val, s=marker_size)\n",
    "axes[0, 2].axhline(y=0, color='r', linestyle='--', lw=2.5)\n",
    "axes[0, 2].set_xlabel('Actual Score (Scaled)', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].set_ylabel('Residuals', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].set_title('BERT: Residuals', fontsize=15, fontweight='bold')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "axes[0, 2].tick_params(labelsize=12)\n",
    "\n",
    "# 2b. Residual Plot - Ridge\n",
    "residuals_ridge = y_pred - y_val\n",
    "axes[0, 3].scatter(y_val, residuals_ridge, alpha=alpha_val, s=marker_size)\n",
    "axes[0, 3].axhline(y=0, color='r', linestyle='--', lw=2.5)\n",
    "axes[0, 3].set_xlabel('Actual Score (Scaled)', fontsize=14, fontweight='bold')\n",
    "axes[0, 3].set_ylabel('Residuals', fontsize=14, fontweight='bold')\n",
    "axes[0, 3].set_title('Ridge: Residuals', fontsize=15, fontweight='bold')\n",
    "axes[0, 3].grid(True, alpha=0.3)\n",
    "axes[0, 3].tick_params(labelsize=12)\n",
    "\n",
    "# 3a. Distribution of Predictions vs Actual - BERT\n",
    "axes[1, 0].hist(labels, bins=30, alpha=0.6, label='Actual', color='blue', edgecolor='black', linewidth=0.5)\n",
    "axes[1, 0].hist(preds, bins=30, alpha=0.6, label='Predicted', color='orange', edgecolor='black', linewidth=0.5)\n",
    "axes[1, 0].set_xlabel('Score (Scaled)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_title('BERT: Distribution', fontsize=15, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=12)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].tick_params(labelsize=12)\n",
    "\n",
    "# 3b. Distribution of Predictions vs Actual - Ridge\n",
    "axes[1, 1].hist(y_val, bins=30, alpha=0.6, label='Actual', color='blue', edgecolor='black', linewidth=0.5)\n",
    "axes[1, 1].hist(y_pred, bins=30, alpha=0.6, label='Predicted', color='orange', edgecolor='black', linewidth=0.5)\n",
    "axes[1, 1].set_xlabel('Score (Scaled)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_title('Ridge: Distribution', fontsize=15, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=12)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].tick_params(labelsize=12)\n",
    "\n",
    "# 4a. Error Distribution - BERT\n",
    "axes[1, 2].hist(residuals_bert, bins=30, color='green', alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "axes[1, 2].axvline(x=0, color='r', linestyle='--', lw=2.5)\n",
    "axes[1, 2].set_xlabel('Prediction Error', fontsize=14, fontweight='bold')\n",
    "axes[1, 2].set_ylabel('Frequency', fontsize=14, fontweight='bold')\n",
    "axes[1, 2].set_title(f'BERT: Error Dist\\nMAE = {mae_b:.4f}', fontsize=15, fontweight='bold')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "axes[1, 2].tick_params(labelsize=12)\n",
    "\n",
    "# 4b. Error Distribution - Ridge\n",
    "axes[1, 3].hist(residuals_ridge, bins=30, color='green', alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "axes[1, 3].axvline(x=0, color='r', linestyle='--', lw=2.5)\n",
    "axes[1, 3].set_xlabel('Prediction Error', fontsize=14, fontweight='bold')\n",
    "axes[1, 3].set_ylabel('Frequency', fontsize=14, fontweight='bold')\n",
    "axes[1, 3].set_title(f'Ridge: Error Dist\\nMAE = {mae:.4f}', fontsize=15, fontweight='bold')\n",
    "axes[1, 3].grid(True, alpha=0.3)\n",
    "axes[1, 3].tick_params(labelsize=12)\n",
    "\n",
    "plt.tight_layout(pad=2.0)\n",
    "plt.savefig('all_comparison.png', dpi=400, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Saved side-by-side BERT and Ridge comparison as 'all_comparison.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb9e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_essay_score_bert(essay_text, model, tokenizer, essay_set, score_ranges):\n",
    "    import torch\n",
    "\n",
    "    essay_text = clean_text(essay_text)\n",
    "    inputs = tokenizer(\n",
    "        essay_text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Ensure model is on MPS if available\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        score_scaled = outputs.logits.squeeze().item()\n",
    "\n",
    "    min_score, max_score = score_ranges[essay_set]\n",
    "    score_orig = score_scaled * (max_score - min_score) + min_score\n",
    "\n",
    "    return round(score_orig, 2)\n",
    "\n",
    "def predict_essay_score_ridge(essay_text, essay_set, vectorizer, ridge_model, score_ranges):\n",
    "    # Clean and vectorize essay\n",
    "    essay_clean = clean_text(essay_text)\n",
    "    X_tfidf = vectorizer.transform([essay_clean])\n",
    "    # Add numeric features\n",
    "    essay_len = len(essay_clean)\n",
    "    word_count = len(essay_clean.split())\n",
    "    avg_word_len = np.mean([len(w) for w in essay_clean.split()]) if word_count > 0 else 0\n",
    "    numeric = np.array([[essay_len, word_count, avg_word_len]])\n",
    "    from scipy.sparse import hstack, csr_matrix\n",
    "    X = hstack([X_tfidf, csr_matrix(numeric)])\n",
    "    # Predict\n",
    "    score_scaled = ridge_model.predict(X)[0]\n",
    "    min_score, max_score = score_ranges[essay_set]\n",
    "    score_orig = score_scaled * (max_score - min_score) + min_score\n",
    "    return round(score_orig, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34100220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "score_ranges = {\n",
    "    1: (0, 10), 2: (1, 6), 3: (0, 3), 4: (0, 3),\n",
    "    5: (0, 4), 6: (0, 4), 7: (2, 24), 8: (10, 60)\n",
    "}\n",
    "essay = \"\"\"Ai is very good for teaching.\n",
    "Ai is very good for teaching.\n",
    "Ai is very good for teaching.\n",
    "Ai is very good for teaching.\n",
    "Ai is very good for teaching.\n",
    "Ai is very good for teaching.\n",
    "Ai is very good for teaching.\n",
    "Ai is very good for teaching.\n",
    "Ai is very good for teaching.\n",
    "Ai is very good for teaching.\n",
    "Ai is very good for teaching.\n",
    "Ai is very good for teaching.\n",
    "\"\"\"\n",
    "\n",
    "essay_set =1  # Change as needed\n",
    "\n",
    "# Choose model: \"bert\" or \"ridge\"\n",
    "model_type = \"ridge\"  # or \"ridge\"\n",
    "\n",
    "if model_type == \"bert\":\n",
    "    predicted_score = predict_essay_score_bert(\n",
    "        essay, model, tokenizer, essay_set, score_ranges\n",
    "    )\n",
    "elif model_type == \"ridge\":\n",
    "    predicted_score = predict_essay_score_ridge(\n",
    "        essay, essay_set, vectorizer, ridge, score_ranges\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"model_type must be 'bert' or 'ridge'\")\n",
    "\n",
    "print(\"Predicted score:\", predicted_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
